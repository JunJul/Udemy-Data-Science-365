{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be37f14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\42128\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f5394",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e118c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "mnist_data, mnist_info = tfds.load('mnist', as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7f94f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=1.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6706c84",
   "metadata": {},
   "source": [
    "### Separate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5583ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_data, mnist_test_data = mnist_data['train'], mnist_data['test']\n",
    "\n",
    "# get the number of train, validation, and test data\n",
    "num_train_samples = mnist_info.splits['train'].num_examples\n",
    "# take 10% from train data\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b12c78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6000, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e272f7",
   "metadata": {},
   "source": [
    "### Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96717049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to scale the data\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "644dc7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_data = mnist_train_data.map(scale)\n",
    "scaled_test_data = mnist_test_data.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94821c97",
   "metadata": {},
   "source": [
    "### Shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4711a8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the shuffle size\n",
    "shuffle_size = 10000\n",
    "\n",
    "# shuffled\n",
    "shuffled_train_data = scaled_train_data.shuffle(shuffle_size)\n",
    "\n",
    "# take 10% data from the train_data to vaidation data\n",
    "validation_data = shuffled_train_data.take(num_validation_samples)\n",
    "train_data = shuffled_train_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f4784f",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d4e9ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "train_data = train_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ad27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_input, validation_tagert = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00daf7",
   "metadata": {},
   "source": [
    "## Modeling and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6dbf6765",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100\n",
    "\n",
    "# create model include inputs, hidden layers, and output\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93efe2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "customized_optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1fb0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "540/540 - 3s - loss: 0.3376 - accuracy: 0.9050 - val_loss: 0.1806 - val_accuracy: 0.9505 - 3s/epoch - 6ms/step\n",
      "Epoch 2/50\n",
      "540/540 - 2s - loss: 0.1358 - accuracy: 0.9601 - val_loss: 0.1197 - val_accuracy: 0.9655 - 2s/epoch - 5ms/step\n",
      "Epoch 3/50\n",
      "540/540 - 2s - loss: 0.0952 - accuracy: 0.9713 - val_loss: 0.1029 - val_accuracy: 0.9715 - 2s/epoch - 4ms/step\n",
      "Epoch 4/50\n",
      "540/540 - 3s - loss: 0.0744 - accuracy: 0.9772 - val_loss: 0.0710 - val_accuracy: 0.9802 - 3s/epoch - 5ms/step\n",
      "Epoch 5/50\n",
      "540/540 - 2s - loss: 0.0591 - accuracy: 0.9823 - val_loss: 0.0569 - val_accuracy: 0.9833 - 2s/epoch - 4ms/step\n",
      "Epoch 6/50\n",
      "540/540 - 2s - loss: 0.0478 - accuracy: 0.9851 - val_loss: 0.0548 - val_accuracy: 0.9812 - 2s/epoch - 4ms/step\n",
      "Epoch 7/50\n",
      "540/540 - 2s - loss: 0.0403 - accuracy: 0.9878 - val_loss: 0.0464 - val_accuracy: 0.9863 - 2s/epoch - 4ms/step\n",
      "Epoch 8/50\n",
      "540/540 - 2s - loss: 0.0336 - accuracy: 0.9899 - val_loss: 0.0435 - val_accuracy: 0.9870 - 2s/epoch - 4ms/step\n",
      "Epoch 9/50\n",
      "540/540 - 3s - loss: 0.0302 - accuracy: 0.9904 - val_loss: 0.0353 - val_accuracy: 0.9888 - 3s/epoch - 5ms/step\n",
      "Epoch 10/50\n",
      "540/540 - 2s - loss: 0.0250 - accuracy: 0.9925 - val_loss: 0.0304 - val_accuracy: 0.9893 - 2s/epoch - 4ms/step\n",
      "Epoch 11/50\n",
      "540/540 - 3s - loss: 0.0217 - accuracy: 0.9930 - val_loss: 0.0230 - val_accuracy: 0.9923 - 3s/epoch - 5ms/step\n",
      "Epoch 12/50\n",
      "540/540 - 2s - loss: 0.0201 - accuracy: 0.9934 - val_loss: 0.0177 - val_accuracy: 0.9928 - 2s/epoch - 4ms/step\n",
      "Epoch 13/50\n",
      "540/540 - 2s - loss: 0.0157 - accuracy: 0.9953 - val_loss: 0.0304 - val_accuracy: 0.9910 - 2s/epoch - 5ms/step\n",
      "Epoch 14/50\n",
      "540/540 - 2s - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.0145 - val_accuracy: 0.9953 - 2s/epoch - 4ms/step\n",
      "Epoch 15/50\n",
      "540/540 - 3s - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.0161 - val_accuracy: 0.9948 - 3s/epoch - 5ms/step\n",
      "Epoch 16/50\n",
      "540/540 - 2s - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0117 - val_accuracy: 0.9963 - 2s/epoch - 5ms/step\n",
      "Epoch 17/50\n",
      "540/540 - 2s - loss: 0.0120 - accuracy: 0.9957 - val_loss: 0.0161 - val_accuracy: 0.9943 - 2s/epoch - 5ms/step\n",
      "Epoch 18/50\n",
      "540/540 - 2s - loss: 0.0091 - accuracy: 0.9973 - val_loss: 0.0074 - val_accuracy: 0.9980 - 2s/epoch - 5ms/step\n",
      "Epoch 19/50\n",
      "540/540 - 2s - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.0136 - val_accuracy: 0.9948 - 2s/epoch - 5ms/step\n",
      "Epoch 20/50\n",
      "540/540 - 2s - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0102 - val_accuracy: 0.9965 - 2s/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b6b2b20be0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration = 50\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n",
    "model.fit(train_data, validation_data=(validation_input, validation_tagert), callbacks=[early_stopping], \n",
    "          epochs=iteration, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a3172",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00247ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 243ms/step - loss: 0.0924 - accuracy: 0.9791\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "985d1cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97.9099988937378"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3637e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
