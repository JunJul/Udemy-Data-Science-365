{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5075413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\42128\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3489439",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d045e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "# tfds.load(name, with_info, as_supervised): loads a dataset from Tensorflow datasets\n",
    "    # as_supervised = True, loads the data in a 2-tuple structure [input, target]\n",
    "    # with_info = True, provides a tuple containing info about version, features, # samples of the dataset\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac91462e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_OptionsDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.uint8, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db8a0ee7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    version=1.0.0,\n",
       "    description='The MNIST database of handwritten digits.',\n",
       "    urls=['https://storage.googleapis.com/cvdf-datasets/mnist/'],\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
       "    }),\n",
       "    total_num_examples=70000,\n",
       "    splits={\n",
       "        'test': 10000,\n",
       "        'train': 60000,\n",
       "    },\n",
       "    supervised_keys=('image', 'label'),\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cd436aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "(28, 28, 1)\n",
      "10\n",
      "@article{lecun2010mnist,\n",
      "  title={MNIST handwritten digit database},\n",
      "  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
      "  journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n",
      "  volume={2},\n",
      "  year={2010}\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mnist_info.splits['train'].num_examples)\n",
    "print(mnist_info.features['image'].shape)\n",
    "print(mnist_info.features['label'].num_classes)\n",
    "print(mnist_info.citation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765c02f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seperate train and test\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "# tf.cast(x,dtype): casts (converts) a variable into a given data type\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7905c",
   "metadata": {},
   "source": [
    "## Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29423154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e99fb1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.map(*function*) applies a custom transformation to a given dataset. \n",
    "    # It takes as input a function which determines the transformation\n",
    "    # image and label to the scale function.\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee09730",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_data = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3180335a",
   "metadata": {},
   "source": [
    "## Shuffling and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff410f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffing =  keeping the same infomation but in a different order\n",
    "# shuffling it can help to ensure that the model is not biased towards any particular class.\n",
    "# if 1 < Buffer_size < num_samples we will be optimizing the computational power\n",
    "Buffer_size = 10000\n",
    "\n",
    "# When we are dealing with enormous datasets, we can't shuffle all data at once\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(Buffer_size)\n",
    "\n",
    "# take the first number of validation samples\n",
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples)\n",
    "# skip the first number of validation samples\n",
    "train_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "adbeccfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdbed212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SkipDataset element_spec=(TensorSpec(shape=(28, 28, 1), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6527a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size = 1 = SGD\n",
    "# batch size = n samples = (single batch) GD\n",
    "# 1 < batch size < n samples = mini-batch GD \n",
    "Batch_size = 100\n",
    "\n",
    "# when batching we find the Average loss and accuracy\n",
    "    # This will add a new column to the tensor would indcate to the model how many smaples it should take in each batch\n",
    "    # contains batches of 100 elements each\n",
    "# train_data contained 54,000 tensors of shape (28,28,1). \n",
    "    #When batching, we split train_data to  batches (groups) of 100 samples, yielding 540 batches of shape (100, 28, 28, 1). \n",
    "# dataset.batch(batch_size) a method that combines the consecutive elements of a dataset into batches\n",
    "train_data = train_data.batch(Batch_size)\n",
    "\n",
    "# The model expects the validation dataset in batch form too\n",
    "# we don't need to conduct back propogation, omly fowawrd propogarion, so take all smaples in a batch at once\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = scaled_test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8df79184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iter() creats an object which can be iterated one element at a time (in a for loop or while loop)\n",
    "# next() loads the next element of an iterable object\n",
    "# the fit function requires validation inputs and validation targets to be separated. \n",
    "    # That's why we use iter() to separate inputs and targets.\n",
    "# iter(validation_data) makes the 'validation_data' object iterable.\n",
    "    # Using next(), we are telling it to load the next batch.\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dc5f77",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffe687",
   "metadata": {},
   "source": [
    "## Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ce9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 900\n",
    "\n",
    "activation_sigmoid = 'sigmoid'\n",
    "activation_relu = 'relu'\n",
    "activation_tanh = 'tanh'\n",
    "\n",
    "# tf.keras.Flatten(original shape) transforms ((flattens) a tensor into a vector)\n",
    "# tf.keras.layers.Dense(output size, kernel_initializer, bias_initializer):takes the inputs provided to the model \n",
    "    # and calculates the dot product of the inputs and the weights and adds the bias\n",
    "    # This is also where we can apply an activation function\n",
    "model = tf.keras.Sequential([\n",
    "    # height: 28pixels, width:28pixels, third dimension: black and white color ranges from 0 to 255\n",
    "    # neural network (actually the first dense layer) gets each input as a vector, \n",
    "        # so we flatten each input to make a vector. The input shape is (28, 28, 1) \n",
    "        # and thus the Flatten layer creates a vector of shape (784,) for each input and passes it to the first dense layer.\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=activation_relu),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation=activation_relu),\n",
    "    #tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    #tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    #tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335abfa7",
   "metadata": {},
   "source": [
    "### Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "222abc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "optimizer_adam='adam'\n",
    "\n",
    "# model.complie(optimizer,loss,metrics): configure the model for training\n",
    "# One-hot encoded/categorical_crossentropy targets are not integers! They are vectors like: [0,1,0]. \n",
    "    # Each value is a scalar (integer), but the target itself is a vector of length 3 (in this case).\n",
    "    # sparse_categorical_crossentropy, means that your target will be the integer \"2\", \n",
    "    # instead of a one-hot encoded one like [0,1,0].\n",
    "model.compile(optimizer=optimizer_adam, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5deab",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f070a",
   "metadata": {},
   "source": [
    "### What happens inside an epoch\n",
    "### 1. At the beginning of each epoch, the training loss will be set to 0\n",
    "### 2. The alogorithm will iterate over a preset number of batches, all from train_data\n",
    "### 3. The weights and biases will be updated as many times as there are batches\n",
    "### 4. We will get a value for the loss function, indicating how the training is going\n",
    "### 5. We will also see a training accuracy\n",
    "### 6. At the end of the epoch, the algorithm will forward propagate the whole validation set\n",
    "## When we reach the maximum number of epochs the training will be over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8498f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 6s - loss: 0.1928 - accuracy: 0.9409 - val_loss: 0.0942 - val_accuracy: 0.9727 - 6s/epoch - 11ms/step\n",
      "Epoch 2/10\n",
      "540/540 - 4s - loss: 0.0758 - accuracy: 0.9762 - val_loss: 0.0612 - val_accuracy: 0.9808 - 4s/epoch - 8ms/step\n",
      "Epoch 3/10\n",
      "540/540 - 6s - loss: 0.0505 - accuracy: 0.9844 - val_loss: 0.0457 - val_accuracy: 0.9857 - 6s/epoch - 11ms/step\n",
      "Epoch 4/10\n",
      "540/540 - 5s - loss: 0.0390 - accuracy: 0.9875 - val_loss: 0.0364 - val_accuracy: 0.9890 - 5s/epoch - 10ms/step\n",
      "Epoch 5/10\n",
      "540/540 - 5s - loss: 0.0300 - accuracy: 0.9907 - val_loss: 0.0268 - val_accuracy: 0.9915 - 5s/epoch - 9ms/step\n",
      "Epoch 6/10\n",
      "540/540 - 5s - loss: 0.0248 - accuracy: 0.9918 - val_loss: 0.0275 - val_accuracy: 0.9913 - 5s/epoch - 10ms/step\n",
      "Epoch 7/10\n",
      "540/540 - 5s - loss: 0.0223 - accuracy: 0.9925 - val_loss: 0.0271 - val_accuracy: 0.9905 - 5s/epoch - 9ms/step\n",
      "Epoch 8/10\n",
      "540/540 - 4s - loss: 0.0190 - accuracy: 0.9941 - val_loss: 0.0283 - val_accuracy: 0.9940 - 4s/epoch - 8ms/step\n",
      "Epoch 9/10\n",
      "540/540 - 5s - loss: 0.0196 - accuracy: 0.9940 - val_loss: 0.0382 - val_accuracy: 0.9875 - 5s/epoch - 10ms/step\n",
      "Epoch 10/10\n",
      "540/540 - 6s - loss: 0.0161 - accuracy: 0.9949 - val_loss: 0.0175 - val_accuracy: 0.9938 - 6s/epoch - 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x242d732d660>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Num_epochs = 10\n",
    "\n",
    "# 540/540: number of a single batch in a batch\n",
    "# #s: time to take to calcualte the loss\n",
    "# loss: loss values and 540 different weights and bias updates one for each batch\n",
    "# accuracy: in the what % of the cases our outputs were equal to the targets\n",
    "# val_loss: is used to determine overfitting\n",
    "# val_accuracy: is the true accuracy of the model because it is the average accuracy across each batch\n",
    "model.fit(train_data, epochs=Num_epochs, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929735be",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87db6e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step - loss: 0.0853 - accuracy: 0.9806\n"
     ]
    }
   ],
   "source": [
    "# forward proporgation through the network\n",
    "# model.evaluate(): returns the loss value and metrics values for the model in 'test mode'\n",
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30ce3326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 98.06%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss,test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db883dfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
